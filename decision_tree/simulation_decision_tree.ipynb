{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1SYJrdBcRBCA7Am8IKGeE8TsSULEojBZ5",
      "authorship_tag": "ABX9TyP6NGdG4xHn6rhNuKGhbO0C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thatswhatmeetcoded/Sentiment-Classification/blob/main/decision_tree/simulation_decision_tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDkLbbG0XxRL",
        "outputId": "d9ed0d5b-fd69-404c-8fac-c64a0dde5c4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Cleaned Text: bad boy\n",
            "Predicted Sentiment: neutral\n",
            "{np.int64(0): 'negative', np.int64(1): 'neutral', np.int64(2): 'positive'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from tkinter.constants import BROWSE\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import joblib\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Download NLTK resources (if needed)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Text cleaning setup\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www.\\S+\", \"\", text)\n",
        "    text = re.sub(r\"@\\w+|#\\w+\", \"\", text)\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "    words = text.split()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "\n",
        "# Paths\n",
        "vec_path = '/content/drive/MyDrive/vectorizers'\n",
        "model_path = '/content/drive/MyDrive/models/random_forest_bow.pkl'\n",
        "pca_path = f\"{vec_path}/svd_bow.pkl\"\n",
        "label_path = '/content/drive/MyDrive/features/y.npy'\n",
        "\n",
        "# Load vectorizer, PCA, model\n",
        "bow_vectorizer = joblib.load(f\"{vec_path}/bow_vectorizer.pkl\")\n",
        "svd_bow = joblib.load(pca_path)\n",
        "model = joblib.load(model_path)\n",
        "\n",
        "# Load label encoder (in case you want to decode prediction)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = joblib.load('/content/drive/MyDrive/vectorizers/label_encoder.pkl')  # Load the saved encoder\n",
        "\n",
        "# Input example\n",
        "new_text = \"i am a bad boy\"\n",
        "\n",
        "# Preprocess\n",
        "cleaned = clean_text(new_text)\n",
        "print(\"Cleaned Text:\", cleaned)\n",
        "\n",
        "# Vectorize\n",
        "X_vec =bow_vectorizer.transform([cleaned])\n",
        "\n",
        "# Apply PCA\n",
        "X_reduced = svd_bow.transform(X_vec)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_reduced)\n",
        "sentiment_label = le.inverse_transform(y_pred)[0]\n",
        "\n",
        "print(\"Predicted Sentiment:\", sentiment_label)\n",
        "\n",
        "print(dict(zip(le.transform(le.classes_), le.classes_)))\n"
      ]
    }
  ]
}